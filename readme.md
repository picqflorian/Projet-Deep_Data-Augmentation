# ğŸš€ Smart Data Augmentation & Model Patching

Ce projet explore l'impact de stratÃ©gies d'augmentation de donnÃ©es actives (Active Data Augmentation) sur l'entraÃ®nement de rÃ©seaux de neurones profonds (ResNet-18 sur CIFAR-10).

Il compare trois approches : **Baseline**, **Max Loss (Active Sampling)** et **Dynamic Curriculum**, et dÃ©montre comment l'augmentation de donnÃ©es peut corriger des biais de modÃ¨le (Model Patching).

## ğŸ“‚ Structure du projet

Voici les deux notebooks principaux de ce dÃ©pÃ´t :

### 1ï¸âƒ£ `demo.ipynb` (Start Here / Ã€ lire) ğŸ‘ˆ
**C'est le notebook de prÃ©sentation.**
* Il contient l'explication thÃ©orique, les visualisations des augmentations et l'analyse des rÃ©sultats.
* Il charge les logs prÃ©-enregistrÃ©s pour afficher les courbes de comparaison instantanÃ©ment.
* **C'est le fichier destinÃ© au lecteur.**

### 2ï¸âƒ£ `main.ipynb` (Training Source)
**C'est le moteur d'entraÃ®nement.**
* Il contient tout le code lourd (Training Loops, Gestion du GPU, Sauvegardes).
* Il a Ã©tÃ© exÃ©cutÃ© pour gÃ©nÃ©rer les modÃ¨les et les logs.
* *Note : Ne lancez ce notebook que si vous souhaitez rÃ©-entraÃ®ner les modÃ¨les depuis zÃ©ro (prend plusieurs heures).*

## ğŸ“Š Dossier `results/`
Ce dossier contient les fichiers `.json` (logs d'entraÃ®nement) et `.pth` (poids des modÃ¨les) gÃ©nÃ©rÃ©s par `main.ipynb` et utilisÃ©s par `demo.ipynb` pour l'affichage.

