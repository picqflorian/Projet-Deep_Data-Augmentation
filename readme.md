# Smart Data Augmentation & Model Patching

Ce projet explore l'impact de strat√©gies d'augmentation de donn√©es actives (Active Data Augmentation) sur l'entra√Ænement de r√©seaux de neurones profonds (ResNet-18 sur CIFAR-10).

Il compare trois approches : **Baseline**, **Max Loss (Active Sampling)** et **Dynamic Curriculum**, et d√©montre comment l'augmentation de donn√©es peut corriger des biais de mod√®le (Model Patching).

## Structure du projet

Voici les deux notebooks principaux de ce d√©p√¥t :

### 1Ô∏è `demo.ipynb` (Start Here / √Ä lire) üëà
**C'est le notebook de pr√©sentation.**
* Il contient l'explication th√©orique, les visualisations des augmentations et l'analyse des r√©sultats.
* Il charge les logs pr√©-enregistr√©s pour afficher les courbes de comparaison instantan√©ment.
* **C'est le fichier destin√© au lecteur.**

### 2Ô∏è `main.ipynb` (Training Source)
**C'est le moteur d'entra√Ænement.**
* Il contient tout le code lourd (Training Loops, Gestion du GPU, Sauvegardes).
* Il a √©t√© ex√©cut√© pour g√©n√©rer les mod√®les et les logs.
* *Note : Ne lancez ce notebook que si vous souhaitez r√©-entra√Æner les mod√®les depuis z√©ro (prend plusieurs heures).*

## Dossier `results/`
Ce dossier contient les fichiers `.json` (logs d'entra√Ænement) et `.pth` (poids des mod√®les) g√©n√©r√©s par `main.ipynb` et utilis√©s par `demo.ipynb` pour l'affichage.

