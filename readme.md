# Smart Data Augmentation & Model Patching

This project explores the impact of **Active Data Augmentation** strategies on the training of deep neural networks (ResNet-18 on CIFAR-10).

It compares three approaches: **Baseline**, **Max Loss (Active Sampling)**, and **Dynamic Curriculum**, and demonstrates how data augmentation can correct model biases (Model Patching).

## Project Structure

Here are the two main notebooks in this repository:

### 1Ô∏è `demo.ipynb` (Start Here) 
**This is the presentation notebook.**
* It contains theoretical explanations, visualizations of the augmentations, and the analysis of the results.
* It loads pre-recorded logs to display comparison plots instantly.
* **This file is intended for the reader/grader.**

### 2 `main.ipynb` (Training Source)
**This is the training engine.**
* It contains all the heavy code (Training Loops, GPU Management, Saving).
* It was executed to generate the models and logs.
* *Note: Only run this notebook if you wish to retrain the models from scratch (takes several hours).*

## `results/` Folder
This folder contains the `.json` files (training logs) and `.pth` files (model weights) generated by `main.ipynb` and used by `demo.ipynb` for display.